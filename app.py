import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse

st.set_page_config(
    page_title="SEOè‡ªå‹•ãƒã‚§ãƒƒã‚«ãƒ¼",
    page_icon="ğŸ”",
    layout="wide"
)

st.title("ğŸ” SEOè‡ªå‹•åˆ¤å®šãƒ»æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("URLã‚’å…¥åŠ›ã™ã‚‹ã ã‘ã§SEOã®å•é¡Œç‚¹ã‚’è‡ªå‹•æ¤œå‡ºã—ã€æ”¹å–„ææ¡ˆã‚’è¡Œã„ã¾ã™ã€‚")

# --- å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ  ---
with st.form(key="seo_form"):
    url_input = st.text_input(
        "ãƒã‚§ãƒƒã‚¯ã™ã‚‹URL",
        placeholder="https://example.com",
        help="SEOã‚’ãƒã‚§ãƒƒã‚¯ã—ãŸã„Webãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
    )
    submit = st.form_submit_button("ğŸš€ SEOãƒã‚§ãƒƒã‚¯é–‹å§‹")

if submit and url_input:
    # URLã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if not url_input.startswith(("http://", "https://")):
        url_input = "https://" + url_input

    with st.spinner("ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦SEOåˆ†æä¸­..."):
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (compatible; SEOChecker/1.0)"
            }
            response = requests.get(url_input, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            st.error(f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.stop()

    st.success(f"å–å¾—å®Œäº†: {url_input}")
    st.divider()

    # --- ãƒã‚§ãƒƒã‚¯é …ç›® ---
    results = []

    # 1. titleã‚¿ã‚°
    title_tag = soup.find("title")
    title_text = title_tag.get_text(strip=True) if title_tag else ""
    title_len = len(title_text)
    if not title_text:
        results.append({"category": "title", "status": "âŒ", "item": "titleã‚¿ã‚°", "detail": "titleã‚¿ã‚°ãŒå­˜åœ¨ã—ã¾ã›ã‚“", "fix": "<title>ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«</title> ã‚’è¿½åŠ ã—ã¦ãã ã•ã„"})
    elif title_len < 20:
        results.append({"category": "title", "status": "âš ï¸", "item": "titleã‚¿ã‚°", "detail": f"titleãŒçŸ­ã™ãã¾ã™ï¼ˆ{title_len}æ–‡å­—ï¼‰", "fix": "20ã€œ60æ–‡å­—ç¨‹åº¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€èª¬æ˜çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã«ã—ã¾ã—ã‚‡ã†"})
    elif title_len > 60:
        results.append({"category": "title", "status": "âš ï¸", "item": "titleã‚¿ã‚°", "detail": f"titleãŒé•·ã™ãã¾ã™ï¼ˆ{title_len}æ–‡å­—ï¼‰", "fix": "60æ–‡å­—ä»¥å†…ã«åã‚ã¦ãã ã•ã„ã€‚æ¤œç´¢çµæœã§åˆ‡ã‚Œã¦ã—ã¾ã„ã¾ã™"})
    else:
        results.append({"category": "title", "status": "âœ…", "item": "titleã‚¿ã‚°", "detail": f"è‰¯å¥½ï¼ˆ{title_len}æ–‡å­—ï¼‰: {title_text[:50]}", "fix": ""})

    # 2. meta description
    meta_desc = soup.find("meta", attrs={"name": "description"})
    desc_text = meta_desc["content"].strip() if meta_desc and meta_desc.get("content") else ""
    desc_len = len(desc_text)
    if not desc_text:
        results.append({"category": "meta", "status": "âŒ", "item": "meta description", "detail": "meta descriptionãŒå­˜åœ¨ã—ã¾ã›ã‚“", "fix": '<meta name="description" content="ãƒšãƒ¼ã‚¸ã®èª¬æ˜æ–‡"> ã‚’è¿½åŠ ã—ã¦ãã ã•ã„'})
    elif desc_len < 50:
        results.append({"category": "meta", "status": "âš ï¸", "item": "meta description", "detail": f"descriptionãŒçŸ­ã™ãã¾ã™ï¼ˆ{desc_len}æ–‡å­—ï¼‰", "fix": "50ã€œ160æ–‡å­—ç¨‹åº¦ã®èª¬æ˜æ–‡ã‚’æ›¸ãã¾ã—ã‚‡ã†"})
    elif desc_len > 160:
        results.append({"category": "meta", "status": "âš ï¸", "item": "meta description", "detail": f"descriptionãŒé•·ã™ãã¾ã™ï¼ˆ{desc_len}æ–‡å­—ï¼‰", "fix": "160æ–‡å­—ä»¥å†…ã«åã‚ã¦ãã ã•ã„"})
    else:
        results.append({"category": "meta", "status": "âœ…", "item": "meta description", "detail": f"è‰¯å¥½ï¼ˆ{desc_len}æ–‡å­—ï¼‰", "fix": ""})

    # 3. H1ã‚¿ã‚°
    h1_tags = soup.find_all("h1")
    h1_count = len(h1_tags)
    if h1_count == 0:
        results.append({"category": "heading", "status": "âŒ", "item": "H1ã‚¿ã‚°", "detail": "H1ã‚¿ã‚°ãŒå­˜åœ¨ã—ã¾ã›ã‚“", "fix": "ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ <h1> ã‚¿ã‚°ã‚’1ã¤è¿½åŠ ã—ã¦ãã ã•ã„"})
    elif h1_count > 1:
        results.append({"category": "heading", "status": "âš ï¸", "item": "H1ã‚¿ã‚°", "detail": f"H1ã‚¿ã‚°ãŒè¤‡æ•°ã‚ã‚Šã¾ã™ï¼ˆ{h1_count}å€‹ï¼‰", "fix": "H1ã‚¿ã‚°ã¯1ãƒšãƒ¼ã‚¸ã«1ã¤ãŒåŸå‰‡ã§ã™"})
    else:
        results.append({"category": "heading", "status": "âœ…", "item": "H1ã‚¿ã‚°", "detail": f"è‰¯å¥½: {h1_tags[0].get_text(strip=True)[:50]}", "fix": ""})

    # 4. ç”»åƒã®altå±æ€§
    images = soup.find_all("img")
    img_no_alt = [img for img in images if not img.get("alt")]
    if img_no_alt:
        results.append({"category": "image", "status": "âš ï¸", "item": "ç”»åƒã®altå±æ€§", "detail": f"{len(img_no_alt)}å€‹ã®ç”»åƒã«altå±æ€§ãŒã‚ã‚Šã¾ã›ã‚“", "fix": "ã™ã¹ã¦ã® <img> ã‚¿ã‚°ã«é©åˆ‡ãª alt å±æ€§ã‚’è¿½åŠ ã—ã¦ãã ã•ã„"})
    elif images:
        results.append({"category": "image", "status": "âœ…", "item": "ç”»åƒã®altå±æ€§", "detail": f"ã™ã¹ã¦ã®ç”»åƒï¼ˆ{len(images)}å€‹ï¼‰ã«altãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™", "fix": ""})
    else:
        results.append({"category": "image", "status": "â„¹ï¸", "item": "ç”»åƒã®altå±æ€§", "detail": "ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "fix": ""})

    # 5. canonical ã‚¿ã‚°
    canonical = soup.find("link", attrs={"rel": "canonical"})
    if canonical:
        results.append({"category": "technical", "status": "âœ…", "item": "canonicalã‚¿ã‚°", "detail": f"è¨­å®šæ¸ˆã¿: {canonical.get('href', '')[:60]}", "fix": ""})
    else:
        results.append({"category": "technical", "status": "âš ï¸", "item": "canonicalã‚¿ã‚°", "detail": "canonicalã‚¿ã‚°ãŒã‚ã‚Šã¾ã›ã‚“", "fix": '<link rel="canonical" href="URL"> ã‚’ <head> å†…ã«è¿½åŠ ã—ã¦ãã ã•ã„'})

    # 6. OGPã‚¿ã‚°
    og_title = soup.find("meta", property="og:title")
    og_desc = soup.find("meta", property="og:description")
    og_image = soup.find("meta", property="og:image")
    ogp_ok = all([og_title, og_desc, og_image])
    if ogp_ok:
        results.append({"category": "social", "status": "âœ…", "item": "OGPã‚¿ã‚°", "detail": "og:title / og:description / og:image ãŒã™ã¹ã¦è¨­å®šã•ã‚Œã¦ã„ã¾ã™", "fix": ""})
    else:
        missing = []
        if not og_title: missing.append("og:title")
        if not og_desc: missing.append("og:description")
        if not og_image: missing.append("og:image")
        results.append({"category": "social", "status": "âš ï¸", "item": "OGPã‚¿ã‚°", "detail": f"ä¸è¶³: {', '.join(missing)}", "fix": "SNSã‚·ã‚§ã‚¢æ™‚ã®è¦‹ãŸç›®ã®ãŸã‚ã« OGP ã‚¿ã‚°ã‚’è¿½åŠ ã—ã¾ã—ã‚‡ã†"})

    # --- çµæœè¡¨ç¤º ---
    ok_count = sum(1 for r in results if r["status"] == "âœ…")
    warn_count = sum(1 for r in results if r["status"] == "âš ï¸")
    ng_count = sum(1 for r in results if r["status"] == "âŒ")

    col1, col2, col3 = st.columns(3)
    col1.metric("âœ… OK", ok_count)
    col2.metric("âš ï¸ è­¦å‘Š", warn_count)
    col3.metric("âŒ NG", ng_count)

    st.subheader("ğŸ“Š ãƒã‚§ãƒƒã‚¯çµæœ")
    for r in results:
        with st.expander(f"{r['status']} {r['item']} â€” {r['detail'][:60]}"):
            st.markdown(f"**è©³ç´°:** {r['detail']}")
            if r["fix"]:
                st.markdown(f"**æ”¹å–„ææ¡ˆ:** {r['fix']}")

elif submit:
    st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
